{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models\n",
    "\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(model, tokenizer, sentence):\n",
    "\n",
    "  # 文を単語に区切って数字にラベル化\n",
    "  tokens = tokenizer(sentence) < meta charset = \"utf-8\" >［\"input_ids\"]\n",
    "\n",
    "      # BERTモデルの処理のためtensor型に変換\n",
    "      input = torch.tensor(tokens).reshape(1, -1)\n",
    "\n",
    "      # BERTモデルに入力し文のベクトルを取得\n",
    "      with torch.no_grad():\n",
    "      outputs= model(input, output_hidden_states=True)\n",
    "      last_hidden_state = outputs.last_hidden_state < meta charset = \"utf-8\" >［0]\n",
    "      averaged_hidden_state = last_hidden_state.sum(dim=0) / len(last_hidden_state)\n",
    "\n",
    "      return averaged_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(model, tokenizer, sentence):\n",
    "    tokens = tokenizer(sentence, add_special_tokens=True)[\"input_ids\"]\n",
    "    input = torch.tensor(tokens).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input)\n",
    "        last_hidden_state = outputs[0]\n",
    "        averaged_hidden_state = last_hidden_state.mean(dim=0)\n",
    "    return averaged_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(model, tokenizer, sentence):\n",
    "    tokens = tokenizer(sentence, add_special_tokens=True)[\"input_ids\"]\n",
    "    input = torch.tensor(tokens).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input)\n",
    "        last_hidden_state = outputs[0]\n",
    "        averaged_hidden_state = last_hidden_state.mean(dim=0).unsqueeze(0)\n",
    "    return averaged_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"我輩は猫である。\"\n",
    "sentence_vector = sentence_to_vector(model, tokenizer, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similarity(sentence1, sentence2):\n",
    "  print(\"{}\\n{}\".format(sentence1, sentence2))\n",
    "\n",
    "  sentence_vector1 = sentence_to_vector(model, tokenizer, sentence1)\n",
    "  sentence_vector2 = sentence_to_vector(model, tokenizer, sentence2)\n",
    "\n",
    "  score = torch.nn.functional.cosine_similarity(\n",
    "      sentence_vector1, sentence_vector2, dim=0).detach().numpy().copy()\n",
    "  print(\"類似度：\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_similarity(sentence1, sentence2):\n",
    "    print(\"{}\\n{}\".format(sentence1, sentence2))\n",
    "\n",
    "    sentence_vector1 = sentence_to_vector(model, tokenizer, sentence1)\n",
    "    sentence_vector2 = sentence_to_vector(model, tokenizer, sentence2)\n",
    "\n",
    "    # Pad the sentence vectors with zeros to make them the same length\n",
    "    max_len = max(sentence_vector1.size(0), sentence_vector2.size(0))\n",
    "    sentence_vector1 = torch.nn.functional.pad(\n",
    "        sentence_vector1, (0, max_len - sentence_vector1.size(0)))\n",
    "    sentence_vector2 = torch.nn.functional.pad(\n",
    "        sentence_vector2, (0, max_len - sentence_vector2.size(0)))\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    score = torch.nn.functional.cosine_similarity(\n",
    "        sentence_vector1, sentence_vector2, dim=0).detach().numpy().copy()\n",
    "    print(\"Similarity:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は猫である\n",
      "私は猫です\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence1 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m吾輩は猫である\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m sentence2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m私は猫です\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m calc_similarity(sentence1, sentence2)\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mcalc_similarity\u001b[0;34m(sentence1, sentence2)\u001b[0m\n\u001b[1;32m     11\u001b[0m sentence_vector2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(\n\u001b[1;32m     12\u001b[0m     sentence_vector2, (\u001b[39m0\u001b[39m, max_len \u001b[39m-\u001b[39m sentence_vector2\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)))\n\u001b[1;32m     14\u001b[0m \u001b[39m# Calculate the cosine similarity\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mcosine_similarity(\n\u001b[1;32m     16\u001b[0m     sentence_vector1, sentence_vector2, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSimilarity:\u001b[39m\u001b[39m\"\u001b[39m, score)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "sentence1 = \"吾輩は猫である\"\n",
    "\n",
    "sentence2 = \"私は猫です\"\n",
    "\n",
    "calc_similarity(sentence1, sentence2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence3 = \"お昼ご飯にカレーが食べたい\"\n",
    "\n",
    "calc_similarity(sentence1, sentence3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4251bd4dd34dd98bdf6c990c128a7244877d69ebc52d9147cbc19662179b2ef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
