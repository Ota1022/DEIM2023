{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- csv と transformers ライブラリがインポートされ、transformers ライブラリから BertTokenizer と BertModel クラスを使用して BERT トークナイザとモデルがロードされています。\n",
    "- 2212.csv ファイルが読み取りモードで開かれ、ファイルを読み取るために CSV リーダー オブジェクトが作成されます。\n",
    "- ヘッダ行は next 関数を用いてスキップされます。\n",
    "- CSV ファイルの行を繰り返し処理するために、for ループが使用されます。各行について、日付、文、ユーザ名の値が抽出される。\n",
    "- 文は、BERT トークナイザーを使用して BERT 入力として符号化され、文の BERT 埋め込みは、BERT モデルを使用して計算されます。\n",
    "-  文の BERT 埋め込みは、モデル出力から抽出される。\n",
    "- 文の埋め込みと参照埋め込みとの間のコサイン類似度は、PyTorchのnnモジュールのCosineSimilarityクラスを使って計算される。\n",
    "- 日付、文、ユーザ名、類似度の値が出力される。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV ファイルの行の値にアクセスする方法に問題があるように思われます。私が提供したコードでは、行の値は、ヘッダー行の列名に対応する row[0] や row[1] などの文字列インデックスを使用してアクセスされます。\n",
    "\n",
    "しかし、CSVファイルの行にヘッダー行がない場合は、最初の列の0から始まる整数のインデックスを使って値にアクセスすることになります。この場合、行の値にアクセスするために、文字列インデックスの代わりに整数インデックスを使用するようにコードを修正する必要があります。\n",
    "\n",
    "以下は、行の値にアクセスするために整数インデックスを使用するようにコードを修正する方法の例です。\n",
    "CSVファイルの行の値にアクセスする方法に問題があるようです。私が提供したコードでは、row[0], row[1], row[3] のような整数のインデックスを使用して値にアクセスしていますが、これはCSVファイルの列に対応するものです。\n",
    "\n",
    "しかし、CSVファイルの行の列数が一定でない場合、行の値にアクセスするために別の方法を使用する必要があるかもしれません。この場合、csv.DictReaderクラスを使用すると、列名をキーとして行の値にアクセスすることができます。\n",
    "\n",
    "以下は、csv.DictReader クラスを使用して行の値にアクセスするようにコードを修正する方法の一例です。\n",
    "\n",
    "提供されたコードでは、モデルオブジェクトは、事前に訓練された BERT モデルを表し、与えられた入力文の表現を計算するために使用することができます。入力文は、まずBERTトークナイザーを使ってBERT互換の入力形式に変換され、次にBERTモデルに渡されて文のBERT埋め込みが計算される。\n",
    "\n",
    "input_idsテンソルとattention_maskテンソルは、それぞれ入力文とアテンションマスクを表す。input_idsテンソルは、入力文のトークン化および符号化表現を含み、attention_maskテンソルは、入力文のどのトークンがモデルによって無視されるべきかを示すために使用される。\n",
    "\n",
    "モデル関数はinput_idsとattention_maskを引数として呼び出され、last_hidden_stateとpooler_outputテンソルが返される。last_hidden_stateテンソルにはモデルの最終的な隠れ状態が、pooler_outputテンソルにはプーリング層の出力（入力文を固定長で表現したもの）が格納される。\n",
    "\n",
    "文埋め込みテンソルは、pooler_outputテンソルの最初の列を選択して作成される。このテンソルには入力文の BERT 埋め込みが含まれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state pooler_output\n"
     ]
    }
   ],
   "source": [
    "# Define the reference sentence\n",
    "reference_sentence = \"This is the reference sentence.\"\n",
    "\n",
    "# Encode the reference sentence as a BERT input\n",
    "reference_input_ids = torch.tensor([tokenizer.encode(reference_sentence)])\n",
    "reference_attention_mask = reference_input_ids.ne(0).float()\n",
    "\n",
    "# Calculate the BERT embedding for the reference sentence\n",
    "with torch.no_grad():\n",
    "    last_hidden_state, pooler_output = model(\n",
    "        reference_input_ids, attention_mask=reference_attention_mask)\n",
    "\n",
    "print(last_hidden_state, pooler_output)\n",
    "# # Extract the BERT embedding for the reference sentence\n",
    "# reference_embedding = pooler_output[:, 0, :]\n",
    "\n",
    "# # Open the CSV file in read mode\n",
    "# with open('2212.csv', 'r') as file:\n",
    "#     # Create a CSV reader object\n",
    "#     reader = csv.DictReader(file)\n",
    "\n",
    "#     # Iterate over the rows in the CSV file\n",
    "#     for row in reader:\n",
    "#         # Extract the date, sentence, and username from the row\n",
    "#         date = row['date']\n",
    "#         sentence = row['sentence']\n",
    "#         username = row['username']\n",
    "\n",
    "#         # Encode the sentence as a BERT input\n",
    "#         input_ids = torch.tensor([tokenizer.encode(sentence)])\n",
    "#         attention_mask = input_ids.ne(0).float()\n",
    "\n",
    "#         # Calculate the BERT embedding for the sentence\n",
    "#         with torch.no_grad():\n",
    "#             last_hidden_state, pooler_output = model(\n",
    "#                 input_ids, attention_mask=attention_mask)\n",
    "\n",
    "#         # Extract the BERT embedding for the sentence\n",
    "#         sentence_embedding = pooler_output[:, 0, :]\n",
    "\n",
    "#         # Calculate the cosine similarity between the embedding and the reference embedding\n",
    "#         cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#         similarity = cosine_similarity(\n",
    "#             sentence_embedding, reference_embedding).item()\n",
    "\n",
    "#         # Print the date, sentence, username, and similarity\n",
    "#         print(date, sentence, username, similarity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4251bd4dd34dd98bdf6c990c128a7244877d69ebc52d9147cbc19662179b2ef6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
